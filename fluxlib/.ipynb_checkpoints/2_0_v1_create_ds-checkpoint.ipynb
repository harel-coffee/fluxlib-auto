{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extracted\\BE-Bra_MF_LAT_51.30761_LON_4.51984.csv\n",
      "203321\n",
      "ok\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import numpy as  np\n",
    "import pandas as pd\n",
    "import sys\n",
    "from scipy.spatial.distance import cosine\n",
    "from scipy import stats\n",
    "from matplotlib import pyplot as plt\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "paths = Path(\"extracted\").glob(r\"*.csv\")\n",
    "paths = list(paths)\n",
    "for p in paths:\n",
    "    p = paths[np.random.randint(len(paths))]\n",
    "    p = paths[26]\n",
    "    print(p)\n",
    "    df = pd.read_csv(p)\n",
    "    df.index = pd.to_datetime(df[\"TIMESTAMP_START\"], format=r\"%Y%m%d%H%M\")\n",
    "    df = df.drop(\"TIMESTAMP_START\", axis = 1)\n",
    "    target_idx = np.random.randint(df.shape[0])\n",
    "    print(target_idx)\n",
    "#     print(df.head(3))\n",
    "#     print(df.tail(3))\n",
    "    break\n",
    "print(\"ok\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity_col_names = [\"TA_ERA\", \"SW_IN_ERA\", \"LW_IN_ERA\", \"VPD_ERA\"]\n",
    "nee_col_name = [\"NEE_VUT_REF\"]\n",
    "# target_idx = 184 # 65 # 1092 # 41869 # 545 # 29179 # 1366 # 44822 # 13240 # 12430 # 13288 # 13240 # 1092 # 61 # 28 # 29 # 685 # 3578\n",
    "\n",
    "# degrade the similarity test if vector to be compared has zeros {\n",
    "target_similarity_vector = df[similarity_col_names].iloc[target_idx, :]\n",
    "similarity_col_names_no_zero = target_similarity_vector[target_similarity_vector != 0].index.tolist()\n",
    "# }\n",
    "\n",
    "# split df to target and \"X\" by similarity columns and nee: {\n",
    "target_similarity_vector = df[similarity_col_names_no_zero].iloc[target_idx, :] # print(target_similarity_vector.name)\n",
    "df_similarity = df[similarity_col_names_no_zero].drop([target_similarity_vector.name]) # print(df_similarity.head(10))\n",
    "\n",
    "target_nee = df[nee_col_name].iloc[target_idx, :] # print(target_nee)\n",
    "nee_column =  df[nee_col_name].drop([target_similarity_vector.name])\n",
    "\n",
    "# Get seasons\n",
    "# example:\n",
    "# >>> [(month%12 + 3)//3 for month in range(1, 13)]\n",
    "# [1, 1, 2, 2, 2, 3, 3, 3, 4, 4, 4, 1]\n",
    "seasons = (df_similarity.index.month%12 + 3) // 3 # print(seasons)\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     NEE_VUT_REF\n",
      "TIMESTAMP_START                 \n",
      "2007-07-02 20:30:00     3.271280\n",
      "2007-07-12 20:30:00     6.050520\n",
      "2007-07-16 20:30:00     4.831890\n",
      "2007-08-06 19:00:00     2.557450\n",
      "2007-08-06 19:30:00    -0.541444\n",
      "2007-08-06 20:00:00     4.041190\n",
      "2007-08-06 21:00:00     4.241440\n",
      "2007-08-06 21:30:00     4.259910\n",
      "2007-08-06 22:00:00     4.239320\n",
      "2007-08-07 21:30:00     4.270200\n",
      "2007-08-07 22:00:00     4.158140\n",
      "2007-08-10 20:30:00     6.466790\n",
      "2007-08-10 20:30:00     6.466790\n",
      "2007-08-10 21:00:00     4.767940\n",
      "2007-08-12 20:30:00     4.601120\n",
      "2007-08-23 20:30:00     4.990960\n",
      "2007-08-24 20:30:00     8.999700\n",
      "2007-08-25 20:30:00     4.860050\n",
      "2007-08-26 20:30:00     4.251460\n",
      "2007-09-07 20:30:00     3.776490\n",
      "NEE_VUT_REF    4.11675\n",
      "Name: 2007-08-06 20:30:00, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "similarity_threshold = 1e-4\n",
    "max_iter = 5\n",
    "night_flag = False\n",
    "# determine if night time or day time\n",
    "hour = target_similarity_vector.name.hour\n",
    "if (hour >= 5) and (hour < 19):\n",
    "    # daytime\n",
    "    hour_tolerance = 6\n",
    "else:\n",
    "    # nighttime\n",
    "    hour_tolerance = 2\n",
    "    night_flag = True\n",
    "\n",
    "the_month = (target_similarity_vector.name.month%12 + 3) // 3\n",
    "refined_idxs = np.where(\n",
    "    (seasons == the_month).tolist() \\\n",
    "        and \\\n",
    "    (np.abs(target_similarity_vector.name - df_similarity.index).seconds/3600 < hour_tolerance).tolist()\n",
    ")[0]\n",
    "df_similarity_refined = df_similarity.iloc[refined_idxs, :] # print(df_similarity_refined)\n",
    "\n",
    "if df_similarity_refined.shape[0] > 1000:\n",
    "    refined_idxs = np.where(\n",
    "        np.abs(target_similarity_vector.name.year - df_similarity_refined.index.year) < 1\n",
    "    )[0]\n",
    "    df_similarity_refined = df_similarity_refined.iloc[refined_idxs, :] # print(df_similarity_refined)\n",
    "del(df_similarity)\n",
    "del(refined_idxs)\n",
    "    \n",
    "# # time_difference_object: (time_stamp - target_similarity_vector.name)\n",
    "#         np.abs(\n",
    "#             (time_stamp - target_similarity_vector.name).days * 48 + \\\n",
    "#                (time_stamp - target_similarity_vector.name).seconds/1800\n",
    "#         ), # unit: 30 mins\n",
    "container = [\n",
    "    [\n",
    "        cosine(target_similarity_vector.values, row.values),\n",
    "        time_stamp,\n",
    "        np.abs(target_similarity_vector.name - time_stamp).seconds/3600, # hours\n",
    "        np.abs((time_stamp - target_similarity_vector.name).days), # days\n",
    "        \n",
    "    ] \n",
    "    for time_stamp, row in df_similarity_refined.iterrows()\n",
    "#     if np.abs(target_similarity_vector.name - time_stamp).seconds/3600 < hour_tolerance\n",
    "]\n",
    "\n",
    "keys = [\n",
    "    lambda x: (x[2], x[3], x[0]), # key order: hours, days, similarity\n",
    "    lambda x: (x[3], x[2], x[0]) # key order: days, hours, similarity\n",
    "]\n",
    "\n",
    "nee_containers = []\n",
    "for key in keys:\n",
    "    sieve = container\n",
    "    sieve.sort(key = key)\n",
    "\n",
    "    sieve = np.array(sieve)\n",
    "    for iter_ in range(max_iter):\n",
    "        idx = np.where(sieve[:, 0] < similarity_threshold)[0]\n",
    "        if idx.shape[0] > 100:\n",
    "            similarity_threshold /= 10\n",
    "        else:\n",
    "            break\n",
    "    try:\n",
    "        assert idx.shape[0] >= 10, \"Sieve shorter than 10...\"\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        idx = np.where(sieve[:, 0] < similarity_threshold * 10)[0]\n",
    "\n",
    "    timestamps = sieve[idx, 1][0: 10]\n",
    "    nee_with_outliers = nee_column.loc[timestamps] # print(nee_with_outliers)\n",
    "    del(sieve)\n",
    "    del(idx)\n",
    "    nee_containers.append(nee_with_outliers)\n",
    "\n",
    "nee_with_outliers = pd.concat(nee_containers).sort_index()\n",
    "print(nee_with_outliers)\n",
    "del(container)\n",
    "del(nee_containers)\n",
    "print(target_nee)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4.26908739] [0.36170524]\n",
      "                     NEE_VUT_REF\n",
      "TIMESTAMP_START                 \n",
      "2007-08-06 20:00:00      4.04119\n",
      "2007-08-06 20:00:00      4.04119\n",
      "2007-08-06 20:00:00      4.04119\n",
      "2007-08-06 21:00:00      4.24144\n",
      "2007-08-06 21:00:00      4.24144\n",
      "2007-08-06 21:00:00      4.24144\n",
      "2007-08-06 21:30:00      4.25991\n",
      "2007-08-06 21:30:00      4.25991\n",
      "2007-08-06 21:30:00      4.25991\n",
      "2007-07-02 20:30:00      3.27128\n",
      "2007-07-16 20:30:00      4.83189\n",
      "2007-08-06 20:00:00      4.04119\n",
      "2007-08-06 21:00:00      4.24144\n",
      "2007-08-06 21:30:00      4.25991\n",
      "2007-08-06 22:00:00      4.23932\n",
      "2007-08-07 21:30:00      4.27020\n",
      "2007-08-07 22:00:00      4.15814\n",
      "2007-08-10 21:00:00      4.76794\n",
      "2007-08-12 20:30:00      4.60112\n",
      "2007-08-23 20:30:00      4.99096\n",
      "2007-08-25 20:30:00      4.86005\n",
      "2007-08-26 20:30:00      4.25146\n",
      "2007-09-07 20:30:00      3.77649\n",
      "0\n",
      "                     NEE_VUT_REF\n",
      "TIMESTAMP_START                 \n",
      "2007-08-06 20:00:00      4.04119\n",
      "2007-08-06 21:00:00      4.24144\n",
      "2007-08-06 21:30:00      4.25991\n",
      "2007-08-06 22:00:00      4.23932\n",
      "2007-08-07 21:30:00      4.27020\n",
      "2007-08-07 22:00:00      4.15814\n",
      "2007-08-26 20:30:00      4.25146\n"
     ]
    }
   ],
   "source": [
    "if night_flag:\n",
    "    threshold_z = 0.8\n",
    "    repeat_time = 3\n",
    "else:\n",
    "    threshold_z = 1\n",
    "    repeat_time = 1\n",
    "max_iter = 2\n",
    "\n",
    "repeat_threshold = [\n",
    "    target_similarity_vector.name - timedelta(hours = 1), \n",
    "    target_similarity_vector.name + timedelta(hours = 1)\n",
    "]\n",
    "repeat_index = nee_with_outliers[\n",
    "        (nee_with_outliers.index > repeat_threshold[0]) &\n",
    "        (nee_with_outliers.index <= repeat_threshold[1])\n",
    "].index\n",
    "nee_with_outliers = pd.concat([\n",
    "    nee_with_outliers.loc[\n",
    "        np.repeat(repeat_index, repeat_time)\n",
    "    ],\n",
    "    nee_with_outliers\n",
    "])\n",
    "\n",
    "# remove outliers using IQR: {\n",
    "# calculate the interquartile range (IQR)\n",
    "# get the first and third quantiles:\n",
    "Q1 = np.quantile(nee_with_outliers, 0.25)\n",
    "Q3 = np.quantile(nee_with_outliers, 0.75)\n",
    "# get IQR\n",
    "IQR = Q3 - Q1 # print(IQR)\n",
    "idx_clean = np.where((nee_with_outliers >= (Q1 - 1.5 * IQR)) & (nee_with_outliers <= (Q3 + 1.5 * IQR)))\n",
    "nee_clean = nee_with_outliers.iloc[idx_clean[0]]\n",
    "# print(nee_clean) # print(idx_clean[0][-1], idx_clean[0].shape[0])\n",
    "# }\n",
    "\n",
    "print(np.mean(nee_clean).values, np.std(nee_clean).values)\n",
    "print(nee_clean)\n",
    "\n",
    "for iter_ in range(max_iter):\n",
    "    print(iter_)\n",
    "    # remove outliers using z-score: {\n",
    "\n",
    "    z = np.abs(stats.zscore(nee_clean)) # print(z)\n",
    "    idx_clean = np.where(z < threshold_z)\n",
    "    nee_clean_temp = nee_clean.iloc[idx_clean[0]]\n",
    "    if nee_clean_temp.__len__() < 5:\n",
    "        threshold_z += 1\n",
    "    else:\n",
    "        nee_clean = nee_clean_temp\n",
    "        break\n",
    "nee_clean = nee_clean.drop_duplicates()\n",
    "print(nee_clean)\n",
    "# assert not (nee_clean.values == 0.).all(), \"All values are equal to zero...\"\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the largest cluster\n",
    "def get_majority(clustering):\n",
    "    # get the cluster lable with takes the largest proportion: {\n",
    "    bin_percent = np.bincount(clustering.labels_) / clustering.labels_.shape[0]\n",
    "    # sort by percentage\n",
    "    label_and_percent = list(zip(np.unique(clustering.labels_), bin_percent))\n",
    "    label_and_percent.sort(key = lambda x: x[1], reverse = True)\n",
    "    # print(label_and_percent)\n",
    "    majority = label_and_percent[0]\n",
    "    del(bin_percent, label_and_percent)\n",
    "    # }\n",
    "    return majority"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans, AffinityPropagation\n",
    "\n",
    "n_cluster = 5\n",
    "majority_cluster_thershold = 0.67\n",
    "max_iter = 5\n",
    "\n",
    "try: \n",
    "    assert nee_clean.shape[0] >= n_cluster, \"Bad point...\"\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    n_cluster = nee_clean.shape[0]\n",
    "\n",
    "# first clustering, AffinityPropagation doesn't need setting n_cluster, but it failed to work.\n",
    "# clustering = AffinityPropagation().fit(nee_clean.values.reshape(-1, 1))\n",
    "clustering = KMeans(n_clusters = n_cluster, random_state = 0).fit(nee_clean.values.reshape(-1, 1))\n",
    "# print(f\"clustering lables: {clustering.labels_}, centers: {clustering.cluster_centers_}\")\n",
    "majority = get_majority(clustering)\n",
    "\n",
    "# if majority cluster is less than threshold, re-clustering via Kmeans by reducing n_cluster\n",
    "n_cluster = clustering.cluster_centers_.shape[0]\n",
    "if n_cluster < max_iter:\n",
    "    max_iter = n_cluster\n",
    "iter_ = 1\n",
    "while (majority[1] < majority_cluster_thershold) and iter_ < max_iter:\n",
    "    n_cluster -= 1\n",
    "    \n",
    "    clustering = KMeans(n_clusters = n_cluster, random_state=0)\n",
    "    clustering.fit(nee_clean.values.reshape(-1, 1)) # print(f\"clustering lables: {clustering.labels_}, centers: {clustering.cluster_centers_}\")\n",
    "    majority = get_majority(clustering) # print(f\"majority: {majority}, iter num: {iter_}\")\n",
    "    iter_ += 1\n",
    "\n",
    "# marjority structure: [label, proportion] \n",
    "major_cluster_idx = np.where(clustering.labels_ == majority[0])\n",
    "major_cluster_timestamp = nee_clean.iloc[major_cluster_idx].index # print(major_cluster_timestamp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     NEE_VUT_REF\n",
      "TIMESTAMP_START                 \n",
      "2007-08-06 21:00:00      4.24144\n",
      "2007-08-06 21:30:00      4.25991\n",
      "2007-08-06 22:00:00      4.23932\n",
      "2007-08-07 21:30:00      4.27020\n",
      "2007-08-26 20:30:00      4.25146\n",
      "       mean  per(%)   prob(%)\n",
      "0  4.251460     0.2  0.402253\n",
      "1  4.256477     0.6  0.195495\n",
      "2  4.241440     0.2  0.402253\n",
      "NEE_VUT_REF    4.11675\n",
      "Name: 2007-08-06 20:30:00, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import make_circles\n",
    "from sklearn.ensemble import RandomTreesEmbedding, ExtraTreesClassifier\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "\n",
    "n_clusters = 3\n",
    "# component number of truncated SVD, equals to cluster number\n",
    "n_components = n_clusters\n",
    "\n",
    "# columns for hash embedding\n",
    "# Xs are variables to learn, y is the nee\n",
    "hash_col_names = [\"TA_ERA\", \"SW_IN_ERA\", \"LW_IN_ERA\", \"VPD_ERA\", \"PA_ERA\", \"P_ERA\", \"WS_ERA\"]\n",
    "X_train = df.loc[major_cluster_timestamp, hash_col_names]\n",
    "y_train = df.loc[major_cluster_timestamp, nee_col_name]\n",
    "print(y_train)\n",
    "\n",
    "# use RandomTreesEmbedding to transform data\n",
    "hasher = RandomTreesEmbedding(n_estimators = 10, random_state = 0, max_depth = 3)\n",
    "X_transformed = hasher.fit_transform(X_train) # print(X_transformed.shape, X_train.shape, y_train.shape)\n",
    "svd = TruncatedSVD(n_components = n_components)\n",
    "X_reduced = svd.fit_transform(X_transformed) # print(X_reduced.shape)\n",
    "\n",
    "# clustering NEE in to n_cluster bins, and get mean and percentage of each bin: {\n",
    "\n",
    "clustering = KMeans(n_clusters = n_clusters, random_state = 0).fit(X_reduced, y_train)\n",
    "# unique_labels is uniqued labels, labels euquals to clustering.labels_, counts are count of each label in order.\n",
    "unique_labels, labels, counts = np.unique(clustering.labels_, return_inverse = True, return_counts = True)\n",
    "# percentage of each bin\n",
    "bin_percent = counts / np.sum(counts)\n",
    "\n",
    "# mean values of each bin\n",
    "# y_train is a 2d array, shape [:, 1] , bitcount accepts 1d array\n",
    "assert (y_train.values.ndim == 2) and (y_train.values.shape[1] == 1)\n",
    "bin_sum = np.bincount(labels, weights = y_train.values.ravel())\n",
    "bin_mean = bin_sum / counts \n",
    "# print(pd.DataFrame({\"mean\": bin_mean, \"per(%)\": bin_percent}))\n",
    "\n",
    "# alternative to do this:\n",
    "# bin_percent = np.bincount(clustering.labels_) / clustering.labels_.shape[0]\n",
    "# df_temp = pd.DataFrame({'nee':y.values.ravel(), 'label':clustering.labels_})\n",
    "# df_bin_mean = df_temp.groupby('label').mean()\n",
    "# }\n",
    "\n",
    "\n",
    "X_target = df[hash_col_names].iloc[target_idx, :]\n",
    "assert (X_target.ndim == 1)\n",
    "target_X_transformed = hasher.transform(X_target.values.reshape(1, -1))\n",
    "target_X_reduced = svd.transform(target_X_transformed)\n",
    "\n",
    "# # Learn a Naive Bayes classifier on the transformed data\n",
    "nb = BernoulliNB()\n",
    "nb.fit(X_reduced, clustering.labels_)\n",
    "y_pred_prob = nb.predict_proba(target_X_reduced)\n",
    "# y_pred_prob is a column vector\n",
    "assert (y_pred_prob.ndim == 2) and (y_pred_prob.shape[0] == 1)\n",
    "print(pd.DataFrame({\"mean\": bin_mean, \"per(%)\": bin_percent, \"prob(%)\": y_pred_prob.ravel()}))\n",
    "print(target_nee)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15754, 2, 9, 9)\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "path_ds = Path(\"test/00test.pkl\")\n",
    "with open(path_ds, \"rb\") as f:\n",
    "    ds = pickle.load(f)\n",
    "\n",
    "path_ts = Path(\"test/temp_ts_AU-Rob.pkl\")\n",
    "with open(path_ts, \"rb\") as f:\n",
    "    ts = pickle.load(f)\n",
    "    \n",
    "Xs = ds[\"Xs\"]\n",
    "ys = ds[\"ys\"]\n",
    "# print(ts)\n",
    "print(Xs.shape)\n",
    "\n",
    "# count_outlier = 0\n",
    "# for count, t in enumerate(ts):\n",
    "#     X = Xs[count]\n",
    "#     y = ys[count]\n",
    "#     dif = np.abs(np.mean(X[:, 0]) - y)\n",
    "#     if dif > 50:\n",
    "#         count_outlier += 1\n",
    "#         print(t, count)\n",
    "#         print(dif)\n",
    "#         print(X, y)\n",
    "\n",
    "# print(f\"{np.round(100 * count_outlier / count, 2)}% bad data...\")      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.4665211870506325 2.6199454941024607 0.6076420300008408 (0.843369576857802, 0.0) 0.6071892744388754\n"
     ]
    }
   ],
   "source": [
    "from math import *\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error \n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "path_ = Path(\"test/BE-Bra_MF_1.csv\")\n",
    "# AR-SLu_MF\n",
    "df = pd.read_csv(path_).drop([\"Unnamed: 0\"], axis = 1)\n",
    "pred = df[\"pred\"]\n",
    "true = df[\"ys\"]\n",
    "rms = sqrt(mean_squared_error(pred, true))\n",
    "mae = mean_absolute_error(pred, true)\n",
    "r2 = r2_score(pred, true)\n",
    "pr = pearsonr(pred, true)\n",
    "adj_r2 = (1 - (1 - r2) * ((pred.shape[0] - 1) / \n",
    "          (pred.shape[0] - 5 - 1)))\n",
    "print(mae, rms, r2, pr, adj_r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
